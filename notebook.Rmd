---
title: 'Homework Assignment #1'
author: "Hornella Fokem Fosso & Jan Lennarz"
date: "1/23/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```



## Exercise 1

In this exercise, it will be simulated for different values of $n$, a path of length $T$ from the VAR($1$) model $$\mathbf{X}_t = A_n\mathbf{X}_{t-1} + \epsilon_t, ~~~ t \in \mathbb{Z},$$ where $(\epsilon_t)  \subset \mathbb{R}$ is s a multivariate standard Gaussian white noise.

```{r include=FALSE}
require(MTS)
set.seed(2301)
```

we perform the simulation for $T=100$ and plot the values of the squared norm corresponding to each $n$.

```{r echo=TRUE, message=FALSE, warning=FALSE}
ns <- c(2,5,10,20) # different values of n
lT <- 100          # length T
sNorm <- numeric(length(ns))  # for storing the squared norms
for (s in 1:length(ns)) {
  n <- ns[s]
  X=matrix(0,nrow=n ,ncol=lT)
  An <- 0.5*diag(n)
  for (i in 1:(n-1)) {
    An[i,i+1] <- 1/5
  }
  for (j in 2:lT) {
    X[,j] <- An %*% X[,(j-1)] + t(t(rnorm(n)))
  }
  Estim <- VAR(t(X),1, output = F)
  A_hat <- Estim$Phi
  B <- A_hat - An
  B <- crossprod(B)
  sNorm[s] <- max(abs(eigen(B)$values))
}

plot(ns, sNorm, xlab = "n", ylab = "squared norm", main = "graph for T = 100")

```

The plot shows that the squared norm increases when n becomes larger. In other words, the estimation error of the  seems to be larger when the dimension of the vector is increased. Let's repeat the experiment for $T=500$ and for $T=1000$

```{r }
ns <- c(2,5,10,20)
lT <- 500
sNorm <- numeric(length(ns))
for (s in 1:length(ns)) {
  n <- ns[s]
  X=matrix(0,nrow=n ,ncol=lT)
  An <- 0.5*diag(n)
  for (i in 1:(n-1)) {
    An[i,i+1] <- 1/5
  }
  for (j in 2:lT) {
    X[,j] <- An %*% X[,(j-1)] + t(t(rnorm(n)))
  }
  Estim <- VAR(t(X),1, output = F)
  A_hat <- Estim$Phi
  B <- A_hat - An
  B <- crossprod(B)
  sNorm[s] <- max(abs(eigen(B)$values))
}

plot(ns, sNorm, xlab = "n", ylab = "squared norm", main = "graph for T = 500")

```



```{r }
ns <- c(2,5,10,20)
lT <- 1000
sNorm <- numeric(length(ns))
for (s in 1:length(ns)) {
  n <- ns[s]
  X=matrix(0,nrow=n ,ncol=lT)
  An <- 0.5*diag(n)
  for (i in 1:(n-1)) {
    An[i,i+1] <- 1/5
  }
  for (j in 2:lT) {
    X[,j] <- An %*% X[,(j-1)] + t(t(rnorm(n)))
  }
  Estim <- VAR(t(X),1, output = F)
  A_hat <- Estim$Phi
  B <- A_hat - An
  B <- crossprod(B)
  sNorm[s] <- max(abs(eigen(B)$values))
}

plot(ns, sNorm, xlab = "n", ylab = "squared norm", main = "graph for T = 1000")

```




```{r}

```


# Exercise 2

Data: The series are of various lengths but all end in 1988.  The data set contains the following series: consumer  price  index,  industrial  production,  nominal  GNP,  velocity,  employment,  interest  rate, nominal wages, GNP deflator, money stock, real GNP, stock prices (S&P500), GNP per capita, realwages, unemployment.

We look only at the GNP per capita, nominal  GNP and the real GNP.

Source: C. R. Nelson and C. I. Plosser (1982), Trends and Random Walks in Macroeconomic Time Series.Journal of Monetary Economics,10, 139â€“162. doi: 10.1016/03043932(82)900125.Formerly in the Journal of Business and Economic Statistics data archive,  currently athttp://korora.econ.yale.edu/phillips/data/np&enp.dat.

## 1.

### Stationarity

```{r echo=FALSE, results='hide'}
require(MTS)
library(tseries)
require(sparsevar)
```

First we read the data and do some preprocessing.
```{r}
data(NelPlo)
gnp <- cbind(1,2,gnp.capita, gnp.nom, gnp.real)
n <- dim(gnp)[1]
```

We will look at 3 different versions of the data: Original, log-transformation, series of differences of the log-transformation.

```{r}
Y_orig <- gnp[,3:5]
Y_log=log(gnp[,3:5])
Y_rate <- Y_log[2:n,] - Y_log[1:(n-1),]
Y_rate <- 100*Y_rate
```

*Original:*
```{r}
par(mfrow=c(2,3))
plot(Y_orig[,1],type="l",xlab="",ylab="Log",main="GNP per Capita")
plot(Y_orig[,2],type="l",xlab="",ylab="Log",main="Nominal GNP")
plot(Y_orig[,3],type="l",xlab="",ylab="Log",main="Real GNP")
acf(Y_orig[,1],main="")
acf(Y_orig[,2],main="")
acf(Y_orig[,3],main="")
```

We see that the original data is not stationary.

*Log-Transformation:*
```{r fig.height=5, fig.width=9}
par(mfrow=c(2,3))
plot(Y_log[,1],type="l",xlab="",ylab="Log",main="GNP per Capita")
plot(Y_log[,2],type="l",xlab="",ylab="Log",main="Nominal GNP")
plot(Y_log[,3],type="l",xlab="",ylab="Log",main="Real GNP")
acf(Y_log[,1],main="")
acf(Y_log[,2],main="")
acf(Y_log[,3],main="")
```

Same goes for the log-transformation.

*Log-Transformation rates:*

```{r fig.height=5, fig.width=9}
par(mfrow=c(2,3))
plot(Y_rate[,1],type="l",xlab="",ylab="GNP per Capita")
plot(Y_rate[,2],type="l",xlab="",ylab="Nominal GNP")
plot(Y_rate[,3],type="l",xlab="",ylab="Real GNP")
acf(Y_rate[,1],main="")
acf(Y_rate[,2],main="")
acf(Y_rate[,3],main="")
```

For the rates we can find stationary for all three GNP series. For all three the autocorrelation vanhishes with a lag of 3 which results in $q=2$ for the MA.

Looking at the partial autocorrelation we find the following:

```{r fig.height=2}
par(mfrow=c(1,3))
pacf(Y_rate[,1], main="GNP per Capita")
pacf(Y_rate[,2], main="Nominal GNP")
pacf(Y_rate[,3], main="Real GNP")
```

The GNP partial autocorrelation vanishes after a lag of 2, which results in $p=1$ for the AR part.

### ARMA
We can create an ARMA model for each series individually.

*GNP per Capita:*
```{r}
arma.1 <- arma(Y_rate[,1], order = c(1, 2))
summary(arma.1)
```
We see that all three coefficients are not significant in this model. It is not a very good explanation for this series.

*Nominal GNP:*
```{r}
arma.2 <- arma(Y_rate[,2], order = c(1, 2))
summary(arma.2)
```
Again the coefficients are not significant.

*Real GNP:*
```{r}
arma.3 <- arma(Y_rate[,3], order = c(1, 2))
summary(arma.3)
```

In conclusion for each ARMA model the fit is not perfect. Especially the model for the Real GNP shows flaws. In terms of the AIC the best model of the three is the one for the second series (Nominal GNP).

## 2.

### VAR(1) model

First we fit a VAR(1) model to the data set. We look only at the first 60 time steps. The 19 remaining time steps will be used at the end to compared the performance of the models.

```{r}
t <- 60
mod=VAR(Y_rate[1:t,],1)
res=mod$residuals
```

We see that the residuals are not too highly correlated. Thus, the independence assumption seems to be justyfied. To ensure that we will now check the WN assumption:

```{r, results='hide', fig.keep='all', fig.height=3}
mq(res,adj=1*3^2)
```

```{r fig.height=3}
par(mfrow=c(1,3))
acf(res[,1], main="ACF of Residuals")
acf(res[,2], main="ACF of Residuals")
acf(res[,3], main="ACF of Residuals")
```


The plots show that the residuals are indeed not significantly correlated. The Ljung-Box plot shows that for some lags we have a quite low p-Value but in general the assumption seems to hold, as one can also see in the ACF plots of the residuals.

Next we can check if there are non-significant coefficients in our model.
```{r results='hide'}
VARorder(Y_rate[1:60,]) 
mod2=refVAR(mod,thres=1.96)
```

To compare the reduced model and the original model we can check the AIC:
```{r}
mod$aic
mod2$aic
```

Considering the AIC and BIC the reduced model performs better than the original one.

```{r message=FALSE, results='hide'}
pred1 <- VARpred(mod,1)
pred2 <- VARpred(mod2,1)
rmse <- rbind(mod1=pred1$rmse, mod2=pred2$rmse)
rownames(rmse) <- c("model1", "model2")
```

```{r echo=FALSE}
rmse
```

We can see that the prediction is better for the full model (mod1). But the difference is rather small. It might make sense to consider the simpler model (mod2) then. For the follwing we will therefore, consider the smaller model when comparing it to the VAR with a LASSO.

## 3. VAR with LASSO

We will now fit a VAR with a L1 penalization (LASSO).

```{r}
mod_lasso=fitVAR(Y_rate[1:t,],p=1,penalty="ENET",method="cv")
res_lasso = mod_lasso$residuals
```


Again we check the WN assumption. 

```{r fig.height=3}
par(mfrow=c(1,3))
acf(res_lasso[,1], main="")
acf(res_lasso[,2], main="")
acf(res_lasso[,3], main="")
```

We see that the White Noise assumption does hold for all three series. The residuals are again not significantly correlated.

### Comparison with the simple VAR

We can now calculate the $MSE$ on the last 19 observations to check which model performs better.

```{r}
# Collecting the coefficients of the VAR with LASSO
coef=mod_lasso$A
A1lasso=coef[[1]]

# Collecting the coefficients of the VAR
A2=mod2$Phi
A2=t(A2)

errorLasso <- 0
errorVar <- 0

for (t_ in (t+1):78){
  errorLasso <- errorLasso+mean((Y_rate[(t_+1),]-Y_rate[t_,]%*%A1lasso)^2);
  errorVar <- errorVar+mean((Y_rate[(t_+1),]-Y_rate[t_,]%*%A2)^2);
}

```

The $MSE$ results are:
```{r echo=FALSE}
print(paste("MSE LASSO:", round(errorLasso, 2)))
print(paste("MSE VAR:", round(errorVar, 2)))
```

We see that the VAR performs better than the LASSO. Therefore, we should consider using the VAR model as the model of choice.








